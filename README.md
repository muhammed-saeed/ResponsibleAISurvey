

### Responsible AI Survey Structure

```
Responsible AI Survey
├── Introduction
├── Background
├── Membership Inference Attack
│   ├── Overview
│   ├── Techniques
│   ├── Countermeasures
├── Model Stealing
│   ├── Overview
│   ├── Case Studies
│   ├── Defense Strategies
├── Aligning LLMs with Human Feedback
│   ├── Reinforcement Learning with Human Feedback (RLHF)
│   ├── Direct Preference Optimization (DPO)
│   ├── Supervised Fine-Tuning
│   ├── Simulated Preference Optimization (SimPO)
├── Training Time Attacks
│   ├── Poisoning Attacks
│   ├── Backdoor Attacks
│   ├── Mitigation Techniques
├── Test Time Attacks
│   ├── Jailbreaking LLMs
│   │   ├── Research Questions
│   │   ├── Attack Methods
│   │   ├── Defenses
│   │   ├── Evaluations
│   ├── Jailbreaking VLMs
│   │   ├── Research Questions
│   │   ├── Attack Methods
│   │   ├── Defenses
│   │   ├── Evaluations
├── Catastrophic Risks in AI Applications
│   ├── Misinformation & Factuality Issues
│   │   ├── Case Studies
│   │   ├── Impact Assessment
│   ├── Deepfake Generation
│   ├── Malware Creation
│   ├── Overview of Risks
│   ├── Potential Scenarios
│   ├── Mitigation and Policy Recommendations
├── Social Bias
│   ├── Sources of Bias
│   ├── Bias Detection
│   ├── Mitigating Bias

```

---

###Table of Contents

```markdown
# Responsible AI Survey

## Table of Contents
1. [Introduction](#introduction)
   - [Key References](#introduction-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
2. [Background](#background)
   - [Key References](#background-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
3. [Membership Inference Attack](#membership-inference-attack)
   - [Overview](#overview)
   - [Techniques](#techniques)
   - [Countermeasures](#countermeasures)
   - [Key References](#membership-inference-attack-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
4. [Model Stealing](#model-stealing)
   - [Overview](#overview-1)
   - [Case Studies](#case-studies)
   - [Defense Strategies](#defense-strategies)
   - [Key References](#model-stealing-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
5. [Aligning LLMs with Human Feedback](#aligning-llms-with-human-feedback)
   - [Reinforcement Learning with Human Feedback (RLHF)](#rlhf)
   - [Direct Preference Optimization (DPO)](#dpo)
   - [Supervised Fine-Tuning](#supervised-fine-tuning)
   - [Simulated Preference Optimization (SimPO)](#simpo)
   - [Key References](#aligning-llms-with-human-feedback-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
6. [Training Time Attacks](#training-time-attacks)
   - [Poisoning Attacks](#poisoning-attacks)
   - [Backdoor Attacks](#backdoor-attacks)
   - [Mitigation Techniques](#mitigation-techniques)
   - [Key References](#training-time-attacks-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
7. [Test Time Attacks](#test-time-attacks)
   - [Jailbreaking LLMs](#jailbreaking-llms)
   - [Jailbreaking VLMs](#jailbreaking-vlms)
   - [Key References](#test-time-attacks-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
8. [Catastrophic Application of LLMs and Misuse of LLMs](#misuse-of-llms)
   - [Misinformation & Factuality Issues](#misinformation-and-factuality-issues)
       - [Case Studies](#case-studies-1)
       - [Impact Assessment](#impact-assessment)
   - [Deepfake Generation](#deepfake-generation)
   - [Malware Creation](#malware-creation)
   - [Key References](#misuse-of-llms-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)
9. [Social Bias](#social-bias)
   - [Sources of Bias](#sources-of-bias)
   - [Bias Detection](#bias-detection)
   - [Mitigating Bias](#mitigating-bias)
   - [Key References](#social-bias-key-references)
       - [Paper Name 1](#) - [Link to Paper](#)
       - [Paper Name 2](#) - [Link to Paper](#)

---

## Aligning LLMs with Human Feedback
### Reinforcement Learning with Human Feedback (RLHF)
Overview and key methodologies of RLHF.

### Direct Preference Optimization (DPO)
Discussion of DPO and its advancements.

### Supervised Fine-Tuning
Exploration of fine-tuning models using human-labeled data.

### Simulated Preference Optimization (SimPO)
Explanation of SimPO and its applications.

### Key References
- [Paper Name 1](#) - [Link to Paper](#)
- [Paper Name 2](#) - [Link to Paper](#)
```

